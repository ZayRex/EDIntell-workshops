{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction_to_Keras-EdIntelligence.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punLo1rFYd23"
      },
      "source": [
        "Welcome everyone, this will be the Introduction to Keras workshop orgainised by EdIntelligence. My name is Naman Makkar and I will be taking you through a tour of one of the most popular deep learning libraries out there.\r\n",
        "In this workshop we will be covering applications of keras in computer vision on the MNIST dataset to carry out handwritten digit recognition using Neural Networks.\r\n",
        "This is the documentation for keras - [Keras](https://keras.io/)\r\n",
        "\r\n",
        "We expect the viewers to have some basic information regarding the functioning of Neural Networks like Forward Propagation, Back propagation, activation functions, the 3 types of layers - Input,Hidden,Output etc.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KvbwjqUOCd4T",
        "outputId": "db64aec3-9e1c-48a1-ae47-ceb5f567709b"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWiZBWLZjExn"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D, Dropout, Dense, Flatten, BatchNormalization, MaxPooling2D\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "from keras.optimizers import Adam\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\r\n",
        "from keras.utils.np_utils import to_categorical\r\n",
        "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ioNh8phEdNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0b002c-968d-4d37-c8b7-7f689cbafd78"
      },
      "source": [
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "dfnKXI3pEpu3",
        "outputId": "71d86426-f865-41ce-e341-e55ba07dbaf8"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.imshow(x_train[4], cmap= plt.cm.binary)\r\n",
        "plt.show()\r\n",
        "x_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0U6Qi2TqXPM"
      },
      "source": [
        "This is the documentation for ImageDataGenerator - [Keras-ImageDataGenerator](https://keras.io/api/preprocessing/image/)\r\n",
        "It is used for the purpose of image augmentation which helps us train our model on a variety of augmented images and ensures that we train on a diverse dataset and do not end up over-generalising on a particular image given in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO71RMrN3VCj"
      },
      "source": [
        "datagen = ImageDataGenerator(rotation_range = 10, # Degree range for random rotations - Random images will be rotated by 10 degrees\r\n",
        "                             zoom_range = 0.10, # Range for a random zoom - Images will be zoomed in the range [1-zoom_range,1+zoom_range] if the zoom_range is given as a float\r\n",
        "                             width_shift_range = 0.1, # Shifts the image right or left by the fraction of the image width specified as float\r\n",
        "                             height_shift_range = 0.1) # Shifts the image up or down by the fraction of the image height specified as float"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYQWJUw_cnRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2fe6277-f660-4c6e-d2df-82e961201f2d"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UawhdVUxh1nV"
      },
      "source": [
        "y_test = y_test.reshape(y_test.shape[0],1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5flvAz7h55p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1208aad-af38-48e4-bf64-c0556d7ee237"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lWaQV1_F1Z3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "633d9b34-e412-40ef-f35f-3b9daec57c52"
      },
      "source": [
        "x_train = tf.keras.utils.normalize(x_train, axis=1)\r\n",
        "x_test = tf.keras.utils.normalize(x_test, axis=1)\r\n",
        "x_train[4].shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrjmQkdKiwdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3628c6e0-6036-4ba6-8e65-3917589a5ac3"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARNC7qsWYmyp"
      },
      "source": [
        "Important to make x_train 4 dimensional since if we take batch size into account in batch gradient descent, \r\n",
        "the 4th dimension will represent the number of samples in the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goXnjGsE8NW7"
      },
      "source": [
        "x_train = x_train.reshape(-1,28,28,1) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiM0iyMt8TJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27142df-af3c-4a56-d270-15faae09be74"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWIPdp4Z8VY6"
      },
      "source": [
        "x_test = x_test.reshape(-1,28,28,1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TA_Nn738ac_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ab1d73-1a91-402a-f181-b8fc02e51235"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csHK1DhDjV9O"
      },
      "source": [
        "y_train = to_categorical(y_train)  \r\n",
        "y_test = to_categorical(y_test)\r\n",
        "#This is done to one-hot encode the training and testing labels"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynUrxul42vPn"
      },
      "source": [
        "x_train2,x_val2,y_train2,y_val2 = train_test_split(x_train,y_train,test_size=0.1) \r\n",
        "# We split our training data into a training and a validation using train_test_split from scikit-learn"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H-WRTp9tlJX"
      },
      "source": [
        "We will now build a fairly simple fully connected neural network\r\n",
        "which is a network in which every neuron in one layer is connected to a neuron in the other layer.\r\n",
        "This is a good resource to understand the differences between a fully connected NN and a Convolutional Neural Network - [Medium - Fully Connected vs CNN](https://medium.com/swlh/fully-connected-vs-convolutional-neural-networks-813ca7bc6ee5#:~:text=%20Fully%20connected%20neural%20network%20%201%20A,such%20networks%20do%20tend%20to%20have...%20More) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWeYd9XRoNzx"
      },
      "source": [
        "model = Sequential() # This allows us to combine a sequence of layers - input layer, output layer and a series of hidden layers into a model\r\n",
        "model.add(Flatten()) # This layer flattens the input being given to it\r\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu',input_shape=(784,))) # Used to make fully connected layers \r\n",
        "model.add(BatchNormalization()) # Used to normalize the inputs of each layer\r\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfxuHdPjx3WT"
      },
      "source": [
        "Why do we need Batch Normalization ? - [Why Batch Normalization](https://www.machinecurve.com/index.php/2020/01/14/what-is-batch-normalization-for-training-neural-networks/)\r\n",
        "\r\n",
        "Batch Normalization is needed to avoid internal covariate shift.\r\n",
        "Covariate shift basically means that if we train our model to map from x to y, if the distribution of x is changed our model will struggle to map from x to y even though the function that maps x to y remains the same.\r\n",
        "\r\n",
        "For example lets say we have a binary classifier neural network that differentaites between cats and not-cats with cats being given the label 0 and not-cat being given the label 1, if we train our model on only black cats and have cats of different colour in our test set, our model will have trouble classifying coloured cats as cats even though the decision boundary between cat and not-cat remains unchanged.\r\n",
        "\r\n",
        "When training a multi-layered deep-neural network due to backpropagation the following is expected to happen - \r\n",
        "\r\n",
        "*   The distribution of input data for some particular layer depends on all the interactions happening in all the upstream layers.\r\n",
        "*   A change in how one or more of the upstream layer(s) process data will change the input distribution for this layer.\r\n",
        "\r\n",
        "This is called a covariate shift since the hyperparameters of that particular layer keep changing with respect to the upstream layers which slows down training.\r\n",
        "\r\n",
        "Batch Normalization normalizes the input of each layer (mean = 0 and standard deviation = 1) which helps in reducing the covariate shift since the hyperparameter of each hidden layer are no longer as sensitive to change via the upstream layers.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9zdNxcoMO3I"
      },
      "source": [
        "ReLU vs Sigmoid which activation function is best for hidden layers ?\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   The derivative of the sigmoid function is always smaller than 1, when we have multiple hidden layers, when we keep multiplying these gradients during forward and backward propagation we end up with a product very close to zero, this is the vanishing gradient problem, on the other hand the gradient of ReLU is 0 for x < 0 and 1 for x > 0 since ReLU = max(x,0), therefore, ReLU is the solution to the vanishing gradient problem.\r\n",
        "*   ReLU is more computationally efficient than sigmoid since it only calculates max(x,0) and doesn't need expensive exponential calculations\r\n",
        "*   In practice it gives better convergence results - [Krizhevsky et al](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ABUpQqsTxtS"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZbbU7xBovV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d21b965-5dd3-4bab-fc2e-155ecefc1a87"
      },
      "source": [
        "model.fit_generator(datagen.flow(x_train2, y_train2, batch_size=128),validation_data = (x_val2,y_val2), epochs=20, verbose=1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "422/422 [==============================] - 19s 43ms/step - loss: 0.7756 - accuracy: 0.7548 - val_loss: 0.2259 - val_accuracy: 0.9335\n",
            "Epoch 2/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.2561 - accuracy: 0.9199 - val_loss: 0.1506 - val_accuracy: 0.9545\n",
            "Epoch 3/20\n",
            "422/422 [==============================] - 18s 41ms/step - loss: 0.2031 - accuracy: 0.9344 - val_loss: 0.1070 - val_accuracy: 0.9682\n",
            "Epoch 4/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.1751 - accuracy: 0.9448 - val_loss: 0.0951 - val_accuracy: 0.9707\n",
            "Epoch 5/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.1653 - accuracy: 0.9479 - val_loss: 0.0873 - val_accuracy: 0.9723\n",
            "Epoch 6/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.1420 - accuracy: 0.9551 - val_loss: 0.0897 - val_accuracy: 0.9728\n",
            "Epoch 7/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.1310 - accuracy: 0.9575 - val_loss: 0.0889 - val_accuracy: 0.9737\n",
            "Epoch 8/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.1241 - accuracy: 0.9608 - val_loss: 0.0825 - val_accuracy: 0.9730\n",
            "Epoch 9/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.1211 - accuracy: 0.9610 - val_loss: 0.0773 - val_accuracy: 0.9752\n",
            "Epoch 10/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.1164 - accuracy: 0.9636 - val_loss: 0.0655 - val_accuracy: 0.9782\n",
            "Epoch 11/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.1114 - accuracy: 0.9641 - val_loss: 0.0647 - val_accuracy: 0.9793\n",
            "Epoch 12/20\n",
            "422/422 [==============================] - 18s 43ms/step - loss: 0.1050 - accuracy: 0.9657 - val_loss: 0.0601 - val_accuracy: 0.9822\n",
            "Epoch 13/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.1028 - accuracy: 0.9674 - val_loss: 0.0625 - val_accuracy: 0.9797\n",
            "Epoch 14/20\n",
            "422/422 [==============================] - 18s 42ms/step - loss: 0.0994 - accuracy: 0.9681 - val_loss: 0.0581 - val_accuracy: 0.9815\n",
            "Epoch 15/20\n",
            "422/422 [==============================] - 19s 44ms/step - loss: 0.0938 - accuracy: 0.9698 - val_loss: 0.0645 - val_accuracy: 0.9802\n",
            "Epoch 16/20\n",
            "422/422 [==============================] - 19s 45ms/step - loss: 0.0868 - accuracy: 0.9718 - val_loss: 0.0551 - val_accuracy: 0.9822\n",
            "Epoch 17/20\n",
            "422/422 [==============================] - 18s 44ms/step - loss: 0.0927 - accuracy: 0.9704 - val_loss: 0.0563 - val_accuracy: 0.9833\n",
            "Epoch 18/20\n",
            "422/422 [==============================] - 18s 44ms/step - loss: 0.0858 - accuracy: 0.9730 - val_loss: 0.0539 - val_accuracy: 0.9827\n",
            "Epoch 19/20\n",
            "422/422 [==============================] - 18s 43ms/step - loss: 0.0813 - accuracy: 0.9736 - val_loss: 0.0473 - val_accuracy: 0.9855\n",
            "Epoch 20/20\n",
            "422/422 [==============================] - 19s 44ms/step - loss: 0.0836 - accuracy: 0.9735 - val_loss: 0.0551 - val_accuracy: 0.9830\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0a050c8c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsTaVr2fAR8W",
        "outputId": "5ea0572c-b868-4d23-e2ee-87b5a51455ca"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test,verbose=1)\r\n",
        "print(test_loss, test_acc)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0502 - accuracy: 0.9831\n",
            "0.050212312489748 0.9830999970436096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sk_4QdPPyik"
      },
      "source": [
        "We can save the model architecture and weights in a file like so - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGLrRaFEhQtk",
        "outputId": "5931e72b-ec16-4926-cbfe-9a4a12a03f66"
      },
      "source": [
        "#save model\r\n",
        "model.save('digit_reader.model')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: digit_reader.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efmAER4JPd7g"
      },
      "source": [
        "We can also just save the model weights instead of saving the entire model architecture like so\r\n",
        "Note that if we just save the weights of our model, we can only use the weights file on a model with the exact same architecture as the model we just saved \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Wau9JUo4PCxa",
        "outputId": "aacf7c55-55bd-4b15-97d8-9ea50f31253d"
      },
      "source": [
        "model.save_weights(filepath='final_weight.h5')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d6fa02c92c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#We can also just save the model weights instead of saving the entire model architecture like so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'final_weight.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6hWNZvZPmIW"
      },
      "source": [
        "model.load_weights('final_weight.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87NG1ncJh_yu"
      },
      "source": [
        "loaded_model = keras.models.load_model('digit_reader.model')"
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}